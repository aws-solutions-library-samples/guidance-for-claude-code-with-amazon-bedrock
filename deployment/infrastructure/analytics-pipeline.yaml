# ABOUTME: This CloudFormation template creates a Kinesis Firehose data pipeline for Claude Code analytics
# ABOUTME: It streams CloudWatch Logs to S3 in Parquet format and sets up Athena for querying user metrics

AWSTemplateFormatVersion: '2010-09-09'
Description: 'Analytics pipeline for Claude Code metrics using Kinesis Firehose and Athena'

Parameters:
  MetricsLogGroup:
    Type: String
    Default: /aws/claude-code/metrics
    Description: CloudWatch Log Group containing Claude Code metrics
    
  DataRetentionDays:
    Type: Number
    Default: 90
    Description: Number of days to retain data in S3 before archiving to Glacier
    
  FirehoseBufferInterval:
    Type: Number
    Default: 900
    MinValue: 60
    MaxValue: 900
    Description: Buffer interval in seconds for Kinesis Firehose
    
  DebugMode:
    Type: String
    Default: 'false'
    AllowedValues: ['true', 'false']
    Description: Enable debug logging in Lambda transformation function

Resources:
  # S3 Bucket for analytics data
  AnalyticsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: ArchiveOldData
            Status: Enabled
            Transitions:
              - TransitionInDays: !Ref DataRetentionDays
                StorageClass: GLACIER
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 7
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Purpose
          Value: Claude Code Analytics Data Lake
        - Key: Stack
          Value: !Ref AWS::StackName

  # S3 Bucket for Athena query results
  AthenaResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldQueryResults
            Status: Enabled
            ExpirationInDays: 7
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Purpose
          Value: Athena Query Results
        - Key: Stack
          Value: !Ref AWS::StackName

  # IAM Role for Kinesis Firehose
  FirehoseDeliveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: FirehoseDeliveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt AnalyticsBucket.Arn
                  - !Sub '${AnalyticsBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:GetBucketLocation'
                  - 's3:ListBucketMultipartUploads'
                  - 's3:PutObject'
                Resource:
                  - !GetAtt AnalyticsBucket.Arn
                  - !Sub '${AnalyticsBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 'logs:PutLogEvents'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'lambda:InvokeFunction'
                Resource: !GetAtt TransformLambda.Arn
              - Effect: Allow
                Action:
                  - 'glue:GetDatabase'
                  - 'glue:GetTable'
                  - 'glue:GetTableVersion'
                  - 'glue:GetTableVersions'
                Resource:
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${GlueDatabase}'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/${GlueDatabase}/metrics'

  # IAM Role for CloudWatch Logs to stream to Firehose
  LogsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub 'logs.${AWS::Region}.amazonaws.com'
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: LogsToFirehosePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'firehose:PutRecord'
                  - 'firehose:PutRecordBatch'
                Resource: !GetAtt MetricsFirehose.Arn

  # Lambda function for transforming logs to Parquet format
  TransformLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  TransformLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-transform-to-parquet'
      Runtime: python3.12
      Handler: index.lambda_handler
      Timeout: 300
      MemorySize: 1024
      Role: !GetAtt TransformLambdaRole.Arn
      Environment:
        Variables:
          DEBUG_MODE: !Ref DebugMode
      Code:
        ZipFile: |
          import base64
          import json
          import gzip
          from datetime import datetime
          import os
          
          DEBUG = os.environ.get('DEBUG_MODE', 'false').lower() == 'true'
          
          def lambda_handler(event, context):
              print(f"[INFO] Lambda invoked with request ID: {context.aws_request_id}")
              print(f"[INFO] Number of records received: {len(event.get('records', []))}")
              
              if DEBUG:
                  # Log first record structure for debugging
                  if event.get('records'):
                      first_record = event['records'][0]
                      print(f"[DEBUG] First record structure: {json.dumps(first_record, indent=2)}")
                      try:
                          decoded_data = base64.b64decode(first_record['data'])
                          print(f"[DEBUG] Decoded data length: {len(decoded_data)} bytes")
                          print(f"[DEBUG] First 500 chars of decoded data: {decoded_data[:500]}")
                      except Exception as e:
                          print(f"[DEBUG] Error decoding first record: {str(e)}")
              
              output = []
              processed_count = 0
              dropped_count = 0
              error_count = 0
              metrics_found_total = 0
              
              for idx, record in enumerate(event.get('records', [])):
                  try:
                      # Decode the data
                      compressed_payload = base64.b64decode(record['data'])
                      
                      # Try to decompress - CloudWatch Logs data is gzipped
                      try:
                          uncompressed_payload = gzip.decompress(compressed_payload)
                          if DEBUG and idx < 3:  # Log first 3 records
                              print(f"[DEBUG] Record {idx} successfully decompressed")
                      except gzip.BadGzipFile:
                          # Data might not be gzipped
                          uncompressed_payload = compressed_payload
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} not gzipped, using raw data")
                      
                      # Try to parse as JSON
                      try:
                          log_data = json.loads(uncompressed_payload)
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} parsed as JSON: {json.dumps(log_data, indent=2)[:1000]}")
                      except json.JSONDecodeError:
                          # Try to parse as string containing JSON
                          log_data = uncompressed_payload.decode('utf-8')
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} as string: {log_data[:500]}")
                      
                      # Handle control messages
                      if isinstance(log_data, dict) and log_data.get('messageType') == 'CONTROL_MESSAGE':
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} is CONTROL_MESSAGE, dropping")
                          output.append({
                              'recordId': record['recordId'],
                              'result': 'Dropped'
                          })
                          dropped_count += 1
                          continue
                      
                      # Handle CloudWatch Logs format
                      if isinstance(log_data, dict) and log_data.get('messageType') == 'DATA_MESSAGE' and 'logEvents' in log_data:
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} is DATA_MESSAGE with {len(log_data.get('logEvents', []))} log events")
                          
                          metrics_found = []
                          
                          for log_event in log_data.get('logEvents', []):
                              try:
                                  message = json.loads(log_event.get('message', '{}'))
                                  
                                  if 'claude_code.token.usage' in message:
                                      transformed = {
                                          'timestamp': log_event.get('timestamp', 0),
                                          'user_id': message.get('user.id', ''),
                                          'session_id': message.get('session.id', ''),
                                          'model': message.get('model', ''),
                                          'type': message.get('type', ''),
                                          'token_usage': float(message.get('claude_code.token.usage', 0)),
                                          'terminal_type': message.get('terminal.type', ''),
                                          'organization_id': message.get('organization.id', ''),
                                          'otel_lib': message.get('OTelLib', ''),
                                          'user_email': message.get('user.email', ''),
                                          'user_account_uuid': message.get('user.account_uuid', '')
                                      }
                                      metrics_found.append(transformed)
                                      metrics_found_total += 1
                              except Exception as e:
                                  if DEBUG:
                                      print(f"[DEBUG] Error processing log event: {str(e)}")
                          
                          if metrics_found:
                              combined_data = '\n'.join(json.dumps(m) for m in metrics_found)
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Ok',
                                  'data': base64.b64encode(combined_data.encode('utf-8')).decode('utf-8')
                              })
                              processed_count += 1
                              if DEBUG:
                                  print(f"[DEBUG] Record {idx} processed with {len(metrics_found)} metrics")
                          else:
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Dropped'
                              })
                              dropped_count += 1
                              if DEBUG:
                                  print(f"[DEBUG] Record {idx} dropped - no metrics found")
                      
                      # Handle direct metric format (not wrapped in CloudWatch Logs format)
                      elif isinstance(log_data, str):
                          # Try to parse each line as JSON
                          metrics_found = []
                          for line in log_data.strip().split('\n'):
                              if not line.strip():
                                  continue
                              try:
                                  message = json.loads(line)
                                  if 'claude_code.token.usage' in message:
                                      transformed = {
                                          'timestamp': message.get('_aws', {}).get('Timestamp', 0),
                                          'user_id': message.get('user.id', ''),
                                          'session_id': message.get('session.id', ''),
                                          'model': message.get('model', ''),
                                          'type': message.get('type', ''),
                                          'token_usage': float(message.get('claude_code.token.usage', 0)),
                                          'terminal_type': message.get('terminal.type', ''),
                                          'organization_id': message.get('organization.id', ''),
                                          'otel_lib': message.get('OTelLib', ''),
                                          'user_email': message.get('user.email', ''),
                                          'user_account_uuid': message.get('user.account_uuid', '')
                                      }
                                      metrics_found.append(transformed)
                                      metrics_found_total += 1
                                      if DEBUG:
                                          print(f"[DEBUG] Found metric in direct format: {transformed}")
                              except Exception as e:
                                  if DEBUG:
                                      print(f"[DEBUG] Error parsing line as JSON: {str(e)}")
                          
                          if metrics_found:
                              combined_data = '\n'.join(json.dumps(m) for m in metrics_found)
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Ok',
                                  'data': base64.b64encode(combined_data.encode('utf-8')).decode('utf-8')
                              })
                              processed_count += 1
                          else:
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Dropped'
                              })
                              dropped_count += 1
                      else:
                          # Unknown format
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} has unknown format, dropping")
                          output.append({
                              'recordId': record['recordId'],
                              'result': 'Dropped'
                          })
                          dropped_count += 1
                          
                  except Exception as e:
                      print(f"[ERROR] Error processing record {idx}: {str(e)}")
                      if DEBUG:
                          import traceback
                          print(f"[DEBUG] Full traceback: {traceback.format_exc()}")
                      output.append({
                          'recordId': record['recordId'],
                          'result': 'ProcessingFailed'
                      })
                      error_count += 1
              
              print(f"[INFO] Processing complete - Processed: {processed_count}, Dropped: {dropped_count}, Errors: {error_count}, Total Metrics: {metrics_found_total}")
              
              return {'records': output}

  # Kinesis Data Firehose
  MetricsFirehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: !Sub '${AWS::StackName}-metrics-stream'
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt AnalyticsBucket.Arn
        Prefix: 'data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: 'error/'
        CompressionFormat: UNCOMPRESSED  # Parquet handles its own compression
        BufferingHints:
          SizeInMBs: 128
          IntervalInSeconds: !Ref FirehoseBufferInterval
        DataFormatConversionConfiguration:
          Enabled: true
          InputFormatConfiguration:
            Deserializer:
              OpenXJsonSerDe: {}
          OutputFormatConfiguration:
            Serializer:
              ParquetSerDe: {}
          SchemaConfiguration:
            DatabaseName: !Ref GlueDatabase
            TableName: !Ref GlueTable
            RoleArn: !GetAtt FirehoseDeliveryRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt TransformLambda.Arn
        RoleARN: !GetAtt FirehoseDeliveryRole.Arn

  # CloudWatch Logs Subscription Filter
  LogsSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      LogGroupName: !Ref MetricsLogGroup
      FilterPattern: ''  # Empty pattern sends all logs
      DestinationArn: !GetAtt MetricsFirehose.Arn
      RoleArn: !GetAtt LogsRole.Arn

  # Glue Database for Athena
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub '${AWS::StackName}_analytics'
        Description: Claude Code analytics database

  # Glue Table with partition projection
  GlueTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: metrics
        TableType: EXTERNAL_TABLE
        PartitionKeys:
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
          - Name: hour
            Type: string
        StorageDescriptor:
          Columns:
            - Name: timestamp
              Type: bigint
            - Name: user_id
              Type: string
            - Name: session_id
              Type: string
            - Name: model
              Type: string
            - Name: type
              Type: string
            - Name: token_usage
              Type: double
            - Name: terminal_type
              Type: string
            - Name: organization_id
              Type: string
            - Name: otel_lib
              Type: string
            - Name: user_email
              Type: string
            - Name: user_account_uuid
              Type: string
          Location: !Sub 's3://${AnalyticsBucket}/data/'
          InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
          OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
          SerdeInfo:
            SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        Parameters:
          'projection.enabled': 'true'
          'projection.year.type': 'integer'
          'projection.year.range': '2024,2030'
          'projection.year.digits': '4'
          'projection.month.type': 'integer'
          'projection.month.range': '01,12'
          'projection.month.digits': '2'
          'projection.day.type': 'integer'
          'projection.day.range': '01,31'
          'projection.day.digits': '2'
          'projection.hour.type': 'integer'
          'projection.hour.range': '00,23'
          'projection.hour.digits': '2'
          'storage.location.template': !Sub 's3://${AnalyticsBucket}/data/year=${!year}/month=${!month}/day=${!day}/hour=${!hour}/'

  # Athena Workgroup
  AthenaWorkgroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: !Sub '${AWS::StackName}-workgroup'
      Description: Workgroup for Claude Code analytics queries
      WorkGroupConfiguration:
        ResultConfiguration:
          OutputLocation: !Sub 's3://${AthenaResultsBucket}/results/'
          EncryptionConfiguration:
            EncryptionOption: SSE_S3
        EnforceWorkGroupConfiguration: true
        PublishCloudWatchMetricsEnabled: true

  # Sample Athena Named Queries
  TopUsersQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: Top Users by Token Usage
      Description: Find top 10 users by token consumption in the last 7 days with user identity attribution
      QueryString: !Sub |
        WITH user_totals AS (
            SELECT 
                user_id,
                user_email,
                organization_id,
                SUM(token_usage) as total_tokens,
                COUNT(DISTINCT session_id) as session_count,
                COUNT(DISTINCT DATE(from_unixtime(timestamp/1000))) as active_days
            FROM "${GlueDatabase}".metrics
            WHERE year >= YEAR(CURRENT_DATE - INTERVAL '7' DAY)
                AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
            GROUP BY user_id, user_email, organization_id
        )
        SELECT 
            user_email,
            organization_id,
            SUBSTR(user_id, 1, 8) || '...' as user_id_short,
            total_tokens,
            session_count,
            active_days,
            ROUND(total_tokens * 0.000015, 2) as estimated_cost_usd
        FROM user_totals
        ORDER BY total_tokens DESC
        LIMIT 10;

  TokenUsageByModelQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: Token Usage by Model and Type
      Description: Analyze token usage patterns by model and token type
      QueryString: !Sub |
        SELECT 
            model,
            type as token_type,
            organization_id,
            SUM(token_usage) as total_tokens,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT user_email) as unique_emails,
            COUNT(DISTINCT session_id) as total_sessions,
            ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM "${GlueDatabase}".metrics
        WHERE year >= YEAR(CURRENT_DATE - INTERVAL '30' DAY)
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        GROUP BY model, type, organization_id
        ORDER BY total_tokens DESC;

  UserActivityPatternQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: User Activity Pattern
      Description: Analyze user activity patterns by hour of day
      QueryString: !Sub |
        SELECT 
            HOUR(from_unixtime(timestamp/1000)) as hour_of_day,
            COUNT(DISTINCT user_id) as active_users,
            COUNT(DISTINCT user_email) as active_emails,
            COUNT(DISTINCT organization_id) as active_orgs,
            SUM(token_usage) as total_tokens,
            AVG(token_usage) as avg_tokens_per_request
        FROM "${GlueDatabase}".metrics
        WHERE year >= YEAR(CURRENT_DATE - INTERVAL '7' DAY)
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
        GROUP BY HOUR(from_unixtime(timestamp/1000))
        ORDER BY hour_of_day;
        
  OrganizationUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: Token Usage by Organization
      Description: Analyze token usage across different organizations
      QueryString: !Sub |
        SELECT 
            organization_id,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT user_email) as unique_emails,
            COUNT(DISTINCT session_id) as total_sessions,
            SUM(CASE WHEN type = 'input' THEN token_usage ELSE 0 END) as input_tokens,
            SUM(CASE WHEN type = 'output' THEN token_usage ELSE 0 END) as output_tokens,
            SUM(token_usage) as total_tokens,
            ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM "${GlueDatabase}".metrics
        WHERE year >= YEAR(CURRENT_DATE - INTERVAL '30' DAY)
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        GROUP BY organization_id
        ORDER BY total_tokens DESC;
        
  EmailDomainUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: Token Usage by Email Domain
      Description: Analyze token usage by email domain (extracted from user.email)
      QueryString: !Sub |
        WITH email_domains AS (
          SELECT 
            user_id,
            user_email,
            REGEXP_EXTRACT(user_email, '@([^@]+)$', 1) as email_domain,
            token_usage
          FROM "${GlueDatabase}".metrics
          WHERE year >= YEAR(CURRENT_DATE - INTERVAL '30' DAY)
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
            AND user_email IS NOT NULL
            AND user_email <> ''
        )
        SELECT 
          email_domain,
          COUNT(DISTINCT user_id) as unique_users,
          COUNT(DISTINCT user_email) as unique_emails,
          SUM(token_usage) as total_tokens,
          ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM email_domains
        GROUP BY email_domain
        ORDER BY total_tokens DESC;
        
  IdentityProviderUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      Name: Usage Analysis by Identity Provider
      Description: Compare usage patterns across different identity providers
      QueryString: !Sub |
        WITH user_providers AS (
          SELECT 
            user_id,
            user_email,
            -- Simple heuristic to guess IdP from email patterns or org structure
            CASE 
              WHEN user_email LIKE '%okta%' THEN 'Okta'
              WHEN user_email LIKE '%auth0%' THEN 'Auth0'
              WHEN user_email LIKE '%amazonaws.com' THEN 'Cognito'
              WHEN organization_id = 'amazon-internal' THEN 'Cognito'
              ELSE 'Unknown'
            END as identity_provider,
            token_usage,
            type
          FROM "${GlueDatabase}".metrics
          WHERE year >= YEAR(CURRENT_DATE - INTERVAL '30' DAY)
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        )
        SELECT 
          identity_provider,
          COUNT(DISTINCT user_id) as unique_users,
          SUM(CASE WHEN type = 'input' THEN token_usage ELSE 0 END) as input_tokens,
          SUM(CASE WHEN type = 'output' THEN token_usage ELSE 0 END) as output_tokens,
          SUM(token_usage) as total_tokens,
          ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM user_providers
        GROUP BY identity_provider
        ORDER BY total_tokens DESC;

Outputs:
  AnalyticsBucketName:
    Description: S3 bucket for analytics data
    Value: !Ref AnalyticsBucket
    Export:
      Name: !Sub '${AWS::StackName}-analytics-bucket'

  AthenaWorkgroupName:
    Description: Athena workgroup for queries
    Value: !Ref AthenaWorkgroup
    Export:
      Name: !Sub '${AWS::StackName}-athena-workgroup'

  AthenaDatabaseName:
    Description: Athena database name
    Value: !Ref GlueDatabase
    Export:
      Name: !Sub '${AWS::StackName}-athena-database'

  AthenaTableName:
    Description: Athena table name
    Value: metrics
    Export:
      Name: !Sub '${AWS::StackName}-athena-table'

  AthenaConsoleUrl:
    Description: Direct link to Athena console with the analytics database
    Value: !Sub 'https://console.aws.amazon.com/athena/home?region=${AWS::Region}#/query-editor?db=${GlueDatabase}&workgroup=${AthenaWorkgroup}'

  SampleQueriesInfo:
    Description: Information about sample queries
    Value: !Sub |
      Sample queries have been created:
      1. "Top Users by Token Usage" - Find top 10 users in last 7 days with email and org attribution
      2. "Token Usage by Model and Type" - Analyze usage by model and token type with org context
      3. "User Activity Pattern" - Show activity patterns by hour of day
      4. "Token Usage by Organization" - Track token usage across organizations
      5. "Token Usage by Email Domain" - Analyze usage patterns by email domain
      6. "Usage Analysis by Identity Provider" - Compare usage between Okta, Auth0, and Cognito