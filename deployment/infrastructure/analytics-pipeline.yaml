AWSTemplateFormatVersion: '2010-09-09'
Description: '( SO9610 ) Analytics pipeline for Claude Code metrics using Kinesis Firehose and Athena'

Parameters:
  MetricsLogGroup:
    Type: String
    Default: /aws/claude-code/metrics
    Description: CloudWatch Log Group containing Claude Code metrics

  DataRetentionDays:
    Type: Number
    Default: 90
    Description: Number of days to retain data in S3 before archiving to Glacier

  FirehoseBufferInterval:
    Type: Number
    Default: 900
    MinValue: 60
    MaxValue: 900
    Description: Buffer interval in seconds for Kinesis Firehose

  DebugMode:
    Type: String
    Default: 'false'
    AllowedValues: ['true', 'false']
    Description: Enable debug logging in Lambda transformation function

Resources:
  # S3 Bucket for analytics data
  AnalyticsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: ArchiveOldData
            Status: Enabled
            Transitions:
              - TransitionInDays: !Ref DataRetentionDays
                StorageClass: GLACIER
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 7
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Purpose
          Value: Claude Code Analytics Data Lake
        - Key: Stack
          Value: !Ref AWS::StackName

  # S3 Bucket for Athena query results
  AthenaResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldQueryResults
            Status: Enabled
            ExpirationInDays: 7
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Purpose
          Value: Athena Query Results
        - Key: Stack
          Value: !Ref AWS::StackName

  # IAM Role for Kinesis Firehose
  FirehoseDeliveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: FirehoseDeliveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt AnalyticsBucket.Arn
                  - !Sub '${AnalyticsBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:GetBucketLocation'
                  - 's3:ListBucketMultipartUploads'
                  - 's3:PutObject'
                Resource:
                  - !GetAtt AnalyticsBucket.Arn
                  - !Sub '${AnalyticsBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 'logs:PutLogEvents'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'lambda:InvokeFunction'
                Resource: !GetAtt TransformLambda.Arn
              - Effect: Allow
                Action:
                  - 'glue:GetDatabase'
                  - 'glue:GetTable'
                  - 'glue:GetTableVersion'
                  - 'glue:GetTableVersions'
                Resource:
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${GlueDatabase}'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/${GlueDatabase}/metrics'

  # IAM Role for CloudWatch Logs to stream to Firehose
  LogsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub 'logs.${AWS::Region}.amazonaws.com'
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: LogsToFirehosePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'firehose:PutRecord'
                  - 'firehose:PutRecordBatch'
                Resource: !GetAtt MetricsFirehose.Arn

  # Lambda function for transforming logs to Parquet format
  TransformLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  TransformLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-transform-to-parquet'
      Runtime: python3.12
      Handler: index.lambda_handler
      Timeout: 300
      MemorySize: 1024
      Role: !GetAtt TransformLambdaRole.Arn
      Environment:
        Variables:
          DEBUG_MODE: !Ref DebugMode
      Code:
        ZipFile: |
          import base64
          import json
          import gzip
          from datetime import datetime
          import os

          DEBUG = os.environ.get('DEBUG_MODE', 'false').lower() == 'true'

          def lambda_handler(event, context):
              print(f"[INFO] Lambda invoked with request ID: {context.aws_request_id}")
              print(f"[INFO] Number of records received: {len(event.get('records', []))}")

              if DEBUG:
                  # Log first record structure for debugging
                  if event.get('records'):
                      first_record = event['records'][0]
                      print(f"[DEBUG] First record structure: {json.dumps(first_record, indent=2)}")
                      try:
                          decoded_data = base64.b64decode(first_record['data'])
                          print(f"[DEBUG] Decoded data length: {len(decoded_data)} bytes")
                          print(f"[DEBUG] First 500 chars of decoded data: {decoded_data[:500]}")
                      except Exception as e:
                          print(f"[DEBUG] Error decoding first record: {str(e)}")

              output = []
              processed_count = 0
              dropped_count = 0
              error_count = 0
              metrics_found_total = 0

              for idx, record in enumerate(event.get('records', [])):
                  try:
                      # Decode the data
                      compressed_payload = base64.b64decode(record['data'])

                      # Try to decompress - CloudWatch Logs data is gzipped
                      try:
                          uncompressed_payload = gzip.decompress(compressed_payload)
                          if DEBUG and idx < 3:  # Log first 3 records
                              print(f"[DEBUG] Record {idx} successfully decompressed")
                      except gzip.BadGzipFile:
                          # Data might not be gzipped
                          uncompressed_payload = compressed_payload
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} not gzipped, using raw data")

                      # Try to parse as JSON
                      try:
                          log_data = json.loads(uncompressed_payload)
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} parsed as JSON: {json.dumps(log_data, indent=2)[:1000]}")
                      except json.JSONDecodeError:
                          # Try to parse as string containing JSON
                          log_data = uncompressed_payload.decode('utf-8')
                          if DEBUG and idx < 3:
                              print(f"[DEBUG] Record {idx} as string: {log_data[:500]}")

                      # Handle control messages
                      if isinstance(log_data, dict) and log_data.get('messageType') == 'CONTROL_MESSAGE':
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} is CONTROL_MESSAGE, dropping")
                          output.append({
                              'recordId': record['recordId'],
                              'result': 'Dropped'
                          })
                          dropped_count += 1
                          continue

                      # Handle CloudWatch Logs format
                      if isinstance(log_data, dict) and log_data.get('messageType') == 'DATA_MESSAGE' and 'logEvents' in log_data:
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} is DATA_MESSAGE with {len(log_data.get('logEvents', []))} log events")

                          metrics_found = []

                          for log_event in log_data.get('logEvents', []):
                              try:
                                  message = json.loads(log_event.get('message', '{}'))

                                  if 'claude_code.token.usage' in message:
                                      transformed = {
                                          'timestamp': log_event.get('timestamp', 0),
                                          'user_id': message.get('user.id', ''),
                                          'session_id': message.get('session.id', ''),
                                          'model': message.get('model', ''),
                                          'type': message.get('type', ''),
                                          'token_usage': float(message.get('claude_code.token.usage', 0)),
                                          'terminal_type': message.get('terminal.type', ''),
                                          'organization_id': message.get('organization.id', ''),
                                          'otel_lib': message.get('OTelLib', ''),
                                          'user_email': message.get('user.email', ''),
                                          'user_account_uuid': message.get('user.account_uuid', '')
                                      }
                                      metrics_found.append(transformed)
                                      metrics_found_total += 1
                              except Exception as e:
                                  if DEBUG:
                                      print(f"[DEBUG] Error processing log event: {str(e)}")

                          if metrics_found:
                              combined_data = '\n'.join(json.dumps(m) for m in metrics_found)
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Ok',
                                  'data': base64.b64encode(combined_data.encode('utf-8')).decode('utf-8')
                              })
                              processed_count += 1
                              if DEBUG:
                                  print(f"[DEBUG] Record {idx} processed with {len(metrics_found)} metrics")
                          else:
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Dropped'
                              })
                              dropped_count += 1
                              if DEBUG:
                                  print(f"[DEBUG] Record {idx} dropped - no metrics found")

                      # Handle direct metric format (not wrapped in CloudWatch Logs format)
                      elif isinstance(log_data, str):
                          # Try to parse each line as JSON
                          metrics_found = []
                          for line in log_data.strip().split('\n'):
                              if not line.strip():
                                  continue
                              try:
                                  message = json.loads(line)
                                  if 'claude_code.token.usage' in message:
                                      transformed = {
                                          'timestamp': message.get('_aws', {}).get('Timestamp', 0),
                                          'user_id': message.get('user.id', ''),
                                          'session_id': message.get('session.id', ''),
                                          'model': message.get('model', ''),
                                          'type': message.get('type', ''),
                                          'token_usage': float(message.get('claude_code.token.usage', 0)),
                                          'terminal_type': message.get('terminal.type', ''),
                                          'organization_id': message.get('organization.id', ''),
                                          'otel_lib': message.get('OTelLib', ''),
                                          'user_email': message.get('user.email', ''),
                                          'user_account_uuid': message.get('user.account_uuid', '')
                                      }
                                      metrics_found.append(transformed)
                                      metrics_found_total += 1
                                      if DEBUG:
                                          print(f"[DEBUG] Found metric in direct format: {transformed}")
                              except Exception as e:
                                  if DEBUG:
                                      print(f"[DEBUG] Error parsing line as JSON: {str(e)}")

                          if metrics_found:
                              combined_data = '\n'.join(json.dumps(m) for m in metrics_found)
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Ok',
                                  'data': base64.b64encode(combined_data.encode('utf-8')).decode('utf-8')
                              })
                              processed_count += 1
                          else:
                              output.append({
                                  'recordId': record['recordId'],
                                  'result': 'Dropped'
                              })
                              dropped_count += 1
                      else:
                          # Unknown format
                          if DEBUG:
                              print(f"[DEBUG] Record {idx} has unknown format, dropping")
                          output.append({
                              'recordId': record['recordId'],
                              'result': 'Dropped'
                          })
                          dropped_count += 1

                  except Exception as e:
                      print(f"[ERROR] Error processing record {idx}: {str(e)}")
                      if DEBUG:
                          import traceback
                          print(f"[DEBUG] Full traceback: {traceback.format_exc()}")
                      output.append({
                          'recordId': record['recordId'],
                          'result': 'ProcessingFailed'
                      })
                      error_count += 1

              print(f"[INFO] Processing complete - Processed: {processed_count}, Dropped: {dropped_count}, Errors: {error_count}, Total Metrics: {metrics_found_total}")

              return {'records': output}

  # Kinesis Data Firehose
  MetricsFirehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: !Sub '${AWS::StackName}-metrics-stream'
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt AnalyticsBucket.Arn
        Prefix: 'data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: 'error/'
        CompressionFormat: UNCOMPRESSED # Parquet handles its own compression
        BufferingHints:
          SizeInMBs: 128
          IntervalInSeconds: !Ref FirehoseBufferInterval
        DataFormatConversionConfiguration:
          Enabled: true
          InputFormatConfiguration:
            Deserializer:
              OpenXJsonSerDe: {}
          OutputFormatConfiguration:
            Serializer:
              ParquetSerDe: {}
          SchemaConfiguration:
            DatabaseName: !Ref GlueDatabase
            TableName: !Ref GlueTable
            RoleARN: !GetAtt FirehoseDeliveryRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt TransformLambda.Arn
        RoleARN: !GetAtt FirehoseDeliveryRole.Arn

  # CloudWatch Logs Subscription Filter
  LogsSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      LogGroupName: !Ref MetricsLogGroup
      FilterName: !Sub '${AWS::StackName}-subscription-filter'
      FilterPattern: '' # Empty pattern sends all logs
      DestinationArn: !GetAtt MetricsFirehose.Arn
      RoleArn: !GetAtt LogsRole.Arn

  # Glue Database for Athena
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub '${AWS::StackName}_analytics'
        Description: Claude Code analytics database

  # Glue Table with partition projection
  GlueTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: metrics
        TableType: EXTERNAL_TABLE
        PartitionKeys:
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
          - Name: hour
            Type: string
        StorageDescriptor:
          Columns:
            - Name: timestamp
              Type: bigint
            - Name: user_id
              Type: string
            - Name: session_id
              Type: string
            - Name: model
              Type: string
            - Name: type
              Type: string
            - Name: token_usage
              Type: double
            - Name: terminal_type
              Type: string
            - Name: organization_id
              Type: string
            - Name: otel_lib
              Type: string
            - Name: user_email
              Type: string
            - Name: user_account_uuid
              Type: string
          Location: !Sub 's3://${AnalyticsBucket}/data/'
          InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
          OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
          SerdeInfo:
            SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        Parameters:
          'projection.enabled': 'true'
          'projection.year.type': 'integer'
          'projection.year.range': '2024,2030'
          'projection.year.digits': '4'
          'projection.month.type': 'integer'
          'projection.month.range': '01,12'
          'projection.month.digits': '2'
          'projection.day.type': 'integer'
          'projection.day.range': '01,31'
          'projection.day.digits': '2'
          'projection.hour.type': 'integer'
          'projection.hour.range': '00,23'
          'projection.hour.digits': '2'
          'storage.location.template': !Sub 's3://${AnalyticsBucket}/data/year=${!year}/month=${!month}/day=${!day}/hour=${!hour}/'

  # Athena Workgroup
  AthenaWorkgroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: !Sub '${AWS::StackName}-workgroup'
      Description: Workgroup for Claude Code analytics queries
      WorkGroupConfiguration:
        ResultConfiguration:
          OutputLocation: !Sub 's3://${AthenaResultsBucket}/results/'
          EncryptionConfiguration:
            EncryptionOption: SSE_S3
        EnforceWorkGroupConfiguration: true
        PublishCloudWatchMetricsEnabled: true

  # Sample Athena Named Queries
  TopUsersQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Top Users by Token Usage
      Description: Find top 10 users by token consumption in the last 7 days with user identity attribution
      QueryString: !Sub |
        WITH user_totals AS (
            SELECT
                user_id,
                user_email,
                organization_id,
                SUM(token_usage) as total_tokens,
                COUNT(DISTINCT session_id) as session_count,
                COUNT(DISTINCT CAST(from_unixtime(timestamp/1000) AS DATE)) as active_days
            FROM "${GlueDatabase}".metrics
            WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
                AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                              LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'))
                AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
            GROUP BY user_id, user_email, organization_id
        )
        SELECT
            user_email,
            organization_id,
            SUBSTR(user_id, 1, 8) || '...' as user_id_short,
            total_tokens,
            session_count,
            active_days,
            ROUND(total_tokens * 0.000015, 2) as estimated_cost_usd
        FROM user_totals
        ORDER BY total_tokens DESC
        LIMIT 10;

  TokenUsageByModelQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Token Usage by Model and Type
      Description: Analyze token usage patterns by model and token type
      QueryString: !Sub |
        SELECT
            model,
            type as token_type,
            organization_id,
            SUM(token_usage) as total_tokens,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT user_email) as unique_emails,
            COUNT(DISTINCT session_id) as total_sessions,
            ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM "${GlueDatabase}".metrics
        WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        GROUP BY model, type, organization_id
        ORDER BY total_tokens DESC;

  UserActivityPatternQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: User Activity Pattern
      Description: Analyze user activity patterns by hour of day
      QueryString: !Sub |
        SELECT
            HOUR(from_unixtime(timestamp/1000)) as hour_of_day,
            COUNT(DISTINCT user_id) as active_users,
            COUNT(DISTINCT user_email) as active_emails,
            COUNT(DISTINCT organization_id) as active_orgs,
            SUM(token_usage) as total_tokens,
            AVG(token_usage) as avg_tokens_per_request
        FROM "${GlueDatabase}".metrics
        WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
        GROUP BY HOUR(from_unixtime(timestamp/1000))
        ORDER BY hour_of_day;

  OrganizationUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Token Usage by Organization
      Description: Analyze token usage across different organizations
      QueryString: !Sub |
        SELECT
            organization_id,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT user_email) as unique_emails,
            COUNT(DISTINCT session_id) as total_sessions,
            SUM(CASE WHEN type = 'input' THEN token_usage ELSE 0 END) as input_tokens,
            SUM(CASE WHEN type = 'output' THEN token_usage ELSE 0 END) as output_tokens,
            SUM(token_usage) as total_tokens,
            ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM "${GlueDatabase}".metrics
        WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        GROUP BY organization_id
        ORDER BY total_tokens DESC;

  EmailDomainUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Token Usage by Email Domain
      Description: Analyze token usage by email domain (extracted from user.email)
      QueryString: !Sub |
        WITH email_domains AS (
          SELECT
            user_id,
            user_email,
            regexp_extract(user_email, '@([^@]+)$', 1) as email_domain,
            token_usage
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
            AND user_email IS NOT NULL
            AND user_email <> ''
        )
        SELECT
          email_domain,
          COUNT(DISTINCT user_id) as unique_users,
          COUNT(DISTINCT user_email) as unique_emails,
          SUM(token_usage) as total_tokens,
          ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM email_domains
        GROUP BY email_domain
        ORDER BY total_tokens DESC;

  TPMAnalysisQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Detailed TPM and RPM Analysis
      Description: Calculate tokens per minute and requests per minute by user and model
      QueryString: !Sub |
        WITH minute_metrics AS (
          SELECT
            date_trunc('minute', from_unixtime(timestamp/1000)) as minute,
            user_email,
            user_id,
            organization_id,
            model,
            SUM(token_usage) as tokens_per_minute,
            COUNT(*) as requests_per_minute
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
          GROUP BY
            date_trunc('minute', from_unixtime(timestamp/1000)),
            user_email,
            user_id,
            organization_id,
            model
        )
        SELECT
          minute,
          user_email,
          organization_id,
          model,
          tokens_per_minute as tpm,
          requests_per_minute as rpm,
          ROUND(tokens_per_minute / 60.0, 2) as avg_tps,
          ROUND(requests_per_minute / 60.0, 2) as avg_rps
        FROM minute_metrics
        ORDER BY minute DESC, tokens_per_minute DESC
        LIMIT 1000;

  UserSessionAnalysisQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: User Session Analysis
      Description: Analyze user sessions with duration and intensity metrics
      QueryString: !Sub |
        WITH session_metrics AS (
          SELECT
            session_id,
            user_email,
            user_id,
            organization_id,
            MIN(from_unixtime(timestamp/1000)) as session_start,
            MAX(from_unixtime(timestamp/1000)) as session_end,
            SUM(token_usage) as total_tokens,
            COUNT(*) as api_calls,
            COUNT(DISTINCT model) as models_used,
            AVG(token_usage) as avg_tokens_per_call
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
            AND session_id IS NOT NULL
            AND session_id <> ''
          GROUP BY session_id, user_email, user_id, organization_id
        )
        SELECT
          user_email,
          organization_id,
          session_id,
          session_start,
          session_end,
          date_diff('minute', session_start, session_end) as session_duration_minutes,
          total_tokens,
          api_calls,
          models_used,
          ROUND(avg_tokens_per_call, 2) as avg_tokens_per_call,
          ROUND(total_tokens / greatest(date_diff('minute', session_start, session_end), 1), 2) as tokens_per_minute,
          ROUND(total_tokens * 0.000015, 4) as session_cost_usd
        FROM session_metrics
        WHERE date_diff('minute', session_start, session_end) > 0
        ORDER BY total_tokens DESC
        LIMIT 100;

  CostAttributionQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Detailed Cost Attribution
      Description: Calculate costs by user, organization, and model with hourly breakdown
      QueryString: !Sub |
        WITH hourly_costs AS (
          SELECT
            date_trunc('hour', from_unixtime(timestamp/1000)) as hour,
            user_email,
            user_id,
            organization_id,
            model,
            type as token_type,
            SUM(token_usage) as tokens,
            -- Approximate pricing per million tokens
            CASE
              WHEN model LIKE '%opus-4-1%' THEN 15.00
              WHEN model LIKE '%opus-4%' THEN 15.00
              WHEN model LIKE '%sonnet-4%' THEN 3.00
              WHEN model LIKE '%sonnet-3-7%' THEN 3.00
              WHEN model LIKE '%haiku-3-5%' THEN 0.25
              ELSE 3.00
            END as price_per_million
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
          GROUP BY
            date_trunc('hour', from_unixtime(timestamp/1000)),
            user_email,
            user_id,
            organization_id,
            model,
            type
        )
        SELECT
          hour,
          user_email,
          organization_id,
          model,
          token_type,
          tokens,
          price_per_million,
          ROUND(tokens * price_per_million / 1000000, 4) as cost_usd,
          SUM(tokens) OVER (PARTITION BY user_email ORDER BY hour) as cumulative_tokens,
          SUM(ROUND(tokens * price_per_million / 1000000, 4)) OVER (PARTITION BY user_email ORDER BY hour) as cumulative_cost_usd
        FROM hourly_costs
        ORDER BY hour DESC, cost_usd DESC;

  PeakUsageAnalysisQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Peak Usage and Rate Limit Analysis
      Description: Identify peak usage periods and potential rate limit issues
      QueryString: !Sub |
        WITH minute_usage AS (
          SELECT
            date_trunc('minute', from_unixtime(timestamp/1000)) as minute,
            user_email,
            model,
            SUM(token_usage) as tpm,
            COUNT(*) as rpm
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
          GROUP BY
            date_trunc('minute', from_unixtime(timestamp/1000)),
            user_email,
            model
        ),
        peak_minutes AS (
          SELECT
            minute,
            user_email,
            model,
            tpm,
            rpm,
            -- Anthropic rate limits (approximate)
            CASE
              WHEN model LIKE '%opus%' THEN 40000
              WHEN model LIKE '%sonnet%' THEN 80000
              WHEN model LIKE '%haiku%' THEN 100000
              ELSE 80000
            END as tpm_limit,
            CASE
              WHEN model LIKE '%opus%' THEN 50
              WHEN model LIKE '%sonnet%' THEN 100
              WHEN model LIKE '%haiku%' THEN 100
              ELSE 100
            END as rpm_limit
          FROM minute_usage
        )
        SELECT
          minute,
          user_email,
          model,
          tpm,
          rpm,
          tpm_limit,
          rpm_limit,
          ROUND(100.0 * tpm / tpm_limit, 2) as tpm_utilization_pct,
          ROUND(100.0 * rpm / rpm_limit, 2) as rpm_utilization_pct,
          CASE
            WHEN tpm >= tpm_limit * 0.9 OR rpm >= rpm_limit * 0.9 THEN 'HIGH_RISK'
            WHEN tpm >= tpm_limit * 0.7 OR rpm >= rpm_limit * 0.7 THEN 'MEDIUM_RISK'
            ELSE 'LOW_RISK'
          END as rate_limit_risk
        FROM peak_minutes
        WHERE tpm > 0 OR rpm > 0
        ORDER BY greatest(tpm_utilization_pct, rpm_utilization_pct) DESC
        LIMIT 100;

  IdentityProviderUsageQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref AthenaWorkgroup
      Name: Usage Analysis by Identity Provider
      Description: Compare usage patterns across different identity providers
      QueryString: !Sub |
        WITH user_providers AS (
          SELECT
            user_id,
            user_email,
            -- Simple heuristic to guess IdP from email patterns or org structure
            CASE
              WHEN user_email LIKE '%okta%' THEN 'Okta'
              WHEN user_email LIKE '%auth0%' THEN 'Auth0'
              WHEN user_email LIKE '%amazonaws.com' THEN 'Cognito'
              WHEN organization_id = 'amazon-internal' THEN 'Cognito'
              ELSE 'Unknown'
            END as identity_provider,
            token_usage,
            type
          FROM "${GlueDatabase}".metrics
          WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
            AND month IN (LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '1' MONTH) AS VARCHAR), 2, '0'),
                          LPAD(CAST(MONTH(CURRENT_DATE - INTERVAL '2' MONTH) AS VARCHAR), 2, '0'))
            AND from_unixtime(timestamp/1000) >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
        )
        SELECT
          identity_provider,
          COUNT(DISTINCT user_id) as unique_users,
          SUM(CASE WHEN type = 'input' THEN token_usage ELSE 0 END) as input_tokens,
          SUM(CASE WHEN type = 'output' THEN token_usage ELSE 0 END) as output_tokens,
          SUM(token_usage) as total_tokens,
          ROUND(SUM(token_usage) * 0.000015, 2) as estimated_cost_usd
        FROM user_providers
        GROUP BY identity_provider
        ORDER BY total_tokens DESC;

Outputs:
  AnalyticsBucketName:
    Description: S3 bucket for analytics data
    Value: !Ref AnalyticsBucket
    Export:
      Name: !Sub '${AWS::StackName}-analytics-bucket'

  AthenaWorkgroupName:
    Description: Athena workgroup for queries
    Value: !Ref AthenaWorkgroup
    Export:
      Name: !Sub '${AWS::StackName}-athena-workgroup'

  AthenaDatabaseName:
    Description: Athena database name
    Value: !Ref GlueDatabase
    Export:
      Name: !Sub '${AWS::StackName}-athena-database'

  AthenaTableName:
    Description: Athena table name
    Value: metrics
    Export:
      Name: !Sub '${AWS::StackName}-athena-table'

  AthenaConsoleUrl:
    Description: Direct link to Athena console with the analytics database
    Value: !Sub 'https://console.aws.amazon.com/athena/home?region=${AWS::Region}#/query-editor?db=${GlueDatabase}&workgroup=${AthenaWorkgroup}'

  SampleQueriesInfo:
    Description: Information about sample queries
    Value: !Sub |
      Enhanced queries have been created for comprehensive usage analysis:

      **Real-Time Metrics:**
      1. "Detailed TPM and RPM Analysis" - Calculate tokens per minute and requests per minute by user and model
      2. "Peak Usage and Rate Limit Analysis" - Identify when you're approaching rate limits

      **User Analytics:**
      3. "Top Users by Token Usage" - Find top 10 users in last 7 days with email and org attribution
      4. "User Session Analysis" - Analyze session duration, intensity, and cost metrics
      5. "Token Usage by Email Domain" - Analyze usage patterns by email domain

      **Cost & Attribution:**
      6. "Detailed Cost Attribution" - Calculate precise costs by user, org, and model with cumulative tracking
      7. "Token Usage by Organization" - Track token usage across organizations

      **Usage Patterns:**
      8. "Token Usage by Model and Type" - Analyze usage by model and token type with org context
      9. "User Activity Pattern" - Show activity patterns by hour of day
      10. "Usage Analysis by Identity Provider" - Compare usage between Okta, Auth0, and Cognito

      These queries help you:
      - Track real-time TPM/RPM for rate limit monitoring
      - Identify heavy users and usage spikes
      - Calculate accurate cost attribution
      - Debug metric collection issues
